{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Content-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>cast</th>\n",
       "      <th>country</th>\n",
       "      <th>date_added</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "      <th>listed_in</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>3%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>João Miguel, Bianca Comparato, Michel Gomes, R...</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>August 14, 2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>4 Seasons</td>\n",
       "      <td>International TV Shows, TV Dramas, TV Sci-Fi &amp;...</td>\n",
       "      <td>In a future where the elite inhabit an island ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s2</td>\n",
       "      <td>Movie</td>\n",
       "      <td>7:19</td>\n",
       "      <td>Jorge Michel Grau</td>\n",
       "      <td>Demián Bichir, Héctor Bonilla, Oscar Serrano, ...</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>December 23, 2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>93 min</td>\n",
       "      <td>Dramas, International Movies</td>\n",
       "      <td>After a devastating earthquake hits Mexico Cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3</td>\n",
       "      <td>Movie</td>\n",
       "      <td>23:59</td>\n",
       "      <td>Gilbert Chan</td>\n",
       "      <td>Tedd Chan, Stella Chung, Henley Hii, Lawrence ...</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>December 20, 2018</td>\n",
       "      <td>2011</td>\n",
       "      <td>R</td>\n",
       "      <td>78 min</td>\n",
       "      <td>Horror Movies, International Movies</td>\n",
       "      <td>When an army recruit is found dead, his fellow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s4</td>\n",
       "      <td>Movie</td>\n",
       "      <td>9</td>\n",
       "      <td>Shane Acker</td>\n",
       "      <td>Elijah Wood, John C. Reilly, Jennifer Connelly...</td>\n",
       "      <td>United States</td>\n",
       "      <td>November 16, 2017</td>\n",
       "      <td>2009</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>80 min</td>\n",
       "      <td>Action &amp; Adventure, Independent Movies, Sci-Fi...</td>\n",
       "      <td>In a postapocalyptic world, rag-doll robots hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s5</td>\n",
       "      <td>Movie</td>\n",
       "      <td>21</td>\n",
       "      <td>Robert Luketic</td>\n",
       "      <td>Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...</td>\n",
       "      <td>United States</td>\n",
       "      <td>January 1, 2020</td>\n",
       "      <td>2008</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>123 min</td>\n",
       "      <td>Dramas</td>\n",
       "      <td>A brilliant group of students become card-coun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  show_id     type  title           director  \\\n",
       "0      s1  TV Show     3%                NaN   \n",
       "1      s2    Movie   7:19  Jorge Michel Grau   \n",
       "2      s3    Movie  23:59       Gilbert Chan   \n",
       "3      s4    Movie      9        Shane Acker   \n",
       "4      s5    Movie     21     Robert Luketic   \n",
       "\n",
       "                                                cast        country  \\\n",
       "0  João Miguel, Bianca Comparato, Michel Gomes, R...         Brazil   \n",
       "1  Demián Bichir, Héctor Bonilla, Oscar Serrano, ...         Mexico   \n",
       "2  Tedd Chan, Stella Chung, Henley Hii, Lawrence ...      Singapore   \n",
       "3  Elijah Wood, John C. Reilly, Jennifer Connelly...  United States   \n",
       "4  Jim Sturgess, Kevin Spacey, Kate Bosworth, Aar...  United States   \n",
       "\n",
       "          date_added  release_year rating   duration  \\\n",
       "0    August 14, 2020          2020  TV-MA  4 Seasons   \n",
       "1  December 23, 2016          2016  TV-MA     93 min   \n",
       "2  December 20, 2018          2011      R     78 min   \n",
       "3  November 16, 2017          2009  PG-13     80 min   \n",
       "4    January 1, 2020          2008  PG-13    123 min   \n",
       "\n",
       "                                           listed_in  \\\n",
       "0  International TV Shows, TV Dramas, TV Sci-Fi &...   \n",
       "1                       Dramas, International Movies   \n",
       "2                Horror Movies, International Movies   \n",
       "3  Action & Adventure, Independent Movies, Sci-Fi...   \n",
       "4                                             Dramas   \n",
       "\n",
       "                                         description  \n",
       "0  In a future where the elite inhabit an island ...  \n",
       "1  After a devastating earthquake hits Mexico Cit...  \n",
       "2  When an army recruit is found dead, his fellow...  \n",
       "3  In a postapocalyptic world, rag-doll robots hi...  \n",
       "4  A brilliant group of students become card-coun...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflix = pd.read_csv('data/netflix_titles.zip')\n",
    "netflix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix['description'] = netflix['description'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = model.fit_transform(netflix['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7787, 17905)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = linear_kernel(feature_matrix, feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(netflix.index,index=netflix['title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    #вычисляем попарные коэффициенты косинусной близости\n",
    "    scores = list(enumerate(cosine_sim[idx]))\n",
    "    #сортируем фильмы на основании коэффициентов косинусной близости по убыванию\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    #выбираем десять наибольших значений косинусной близости; нулевую не берём, т. к. это тот же фильм\n",
    "    scores =   scores[1:11]\n",
    "    #забираем индексы\n",
    "    ind_movie = [i[0] for i in scores]\n",
    "    #возвращаем названия по индексам\n",
    "    return netflix['title'].iloc[ind_movie]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5788             Star Trek: The Next Generation\n",
       "5787                      Star Trek: Enterprise\n",
       "5786                 Star Trek: Deep Space Nine\n",
       "5557                     She's Out of My League\n",
       "134                                  7 Days Out\n",
       "6664                        The Midnight Gospel\n",
       "6023                                     Teresa\n",
       "4863    Pinkfong & Baby Shark's Space Adventure\n",
       "5104                                       Rats\n",
       "5970                             Tales by Light\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(\"Star Trek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709                Balto 2: Wolf Quest\n",
       "7446                           Vroomiz\n",
       "1338    Chilling Adventures of Sabrina\n",
       "7388                          Vampires\n",
       "1770                          Dinotrux\n",
       "2767                     Hold the Dark\n",
       "5540                 Shanghai Fortress\n",
       "4041                             Mercy\n",
       "2582                       Half & Half\n",
       "1365        Christmas in the Heartland\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(\"Balto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Коллаборативная фильтрация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Using cached scikit_surprise-1.1.4.tar.gz (154 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from scikit-surprise) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from scikit-surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from scikit-surprise) (1.13.1)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (pyproject.toml): started\n",
      "  Building wheel for scikit-surprise (pyproject.toml): finished with status 'error'\n",
      "Failed to build scikit-surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for scikit-surprise (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [115 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\dump.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\reader.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\utils.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      creating build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "      copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "      copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "      copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "      copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "      creating build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      running egg_info\n",
      "      writing scikit_surprise.egg-info\\PKG-INFO\n",
      "      writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "      writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "      writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "      writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "      dependency C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "      reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      warning: no previously-included files matching '*.so' found under directory 'surprise'\n",
      "      adding license file 'LICENSE.md'\n",
      "      writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "      C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-0tppy7wj\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              ############################\n",
      "              # Package would be ignored #\n",
      "              ############################\n",
      "              Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
      "              but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "              package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
      "              to the `packages` configuration field.\n",
      "      \n",
      "              Alternatively, you can also rely on setuptools' discovery methods\n",
      "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "              instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "              You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "              If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
      "              already explicitly excluding 'surprise.prediction_algorithms' via\n",
      "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "              combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "              You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                    even if it does not contain any `.py` files.\n",
      "                    On the other hand, currently there is no concept of package data\n",
      "                    directory, all directories are treated like packages.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        check.warn(importable)\n",
      "      copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "      copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "      running build_ext\n",
      "      building 'surprise.similarities' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (scikit-surprise)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Reader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BUILTIN_DATASETS \u001b[38;5;66;03m#с помощью данного объекта мы можем использовать встроенные датасеты\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.dataset import BUILTIN_DATASETS #с помощью данного объекта мы можем использовать встроенные датасеты\n",
    "\n",
    "data = Dataset.load_from_file(\n",
    "    \"u.data.txt\",\n",
    "    reader=Reader(line_format=\"user item rating timestamp\", sep=\"\\t\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightfm\n",
      "  Downloading lightfm-1.17.tar.gz (316 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [20 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Projects\\DS\\lessons\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"c:\\Projects\\DS\\lessons\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"c:\\Projects\\DS\\lessons\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-xr6drqis\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-xr6drqis\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-xr6drqis\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\zhigu\\AppData\\Local\\Temp\\pip-build-env-xr6drqis\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 11, in <module>\n",
      "      AttributeError: 'dict' object has no attribute '__LIGHTFM_SETUP__'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "! pip install lightfm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached h5py-3.12.1-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: rich in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.8.0)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached optree-0.13.1-cp311-cp311-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\projects\\ds\\lessons\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl (390.2 MB)\n",
      "   ---------------------------------------- 0.0/390.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/390.2 MB 11.2 MB/s eta 0:00:35\n",
      "   ---------------------------------------- 3.4/390.2 MB 11.2 MB/s eta 0:00:35\n",
      "    --------------------------------------- 6.0/390.2 MB 11.5 MB/s eta 0:00:34\n",
      "    --------------------------------------- 8.4/390.2 MB 11.5 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 10.5/390.2 MB 11.5 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 12.8/390.2 MB 11.5 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 15.2/390.2 MB 11.5 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 17.6/390.2 MB 11.7 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 20.2/390.2 MB 11.6 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 22.5/390.2 MB 11.6 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 24.9/390.2 MB 11.7 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 27.3/390.2 MB 11.7 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 29.6/390.2 MB 11.7 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 32.0/390.2 MB 11.7 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 34.3/390.2 MB 11.7 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 37.0/390.2 MB 11.7 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 39.3/390.2 MB 11.7 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 41.7/390.2 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 44.0/390.2 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 46.4/390.2 MB 11.7 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 49.0/390.2 MB 11.7 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 51.6/390.2 MB 11.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 54.0/390.2 MB 11.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 56.4/390.2 MB 11.7 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 59.0/390.2 MB 11.7 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 61.3/390.2 MB 11.7 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 63.7/390.2 MB 11.7 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 66.1/390.2 MB 11.7 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 68.7/390.2 MB 11.7 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 71.0/390.2 MB 11.7 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 73.7/390.2 MB 11.7 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 76.3/390.2 MB 11.7 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 78.9/390.2 MB 11.7 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 81.3/390.2 MB 11.7 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 83.9/390.2 MB 11.7 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 86.2/390.2 MB 11.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 88.9/390.2 MB 11.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 91.5/390.2 MB 11.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 94.1/390.2 MB 11.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 96.5/390.2 MB 11.7 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 99.1/390.2 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 101.4/390.2 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 104.1/390.2 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 106.7/390.2 MB 11.7 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 109.3/390.2 MB 11.7 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 111.7/390.2 MB 11.7 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 113.2/390.2 MB 11.7 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 115.9/390.2 MB 11.7 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 118.0/390.2 MB 11.6 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 120.1/390.2 MB 11.6 MB/s eta 0:00:24\n",
      "   ------------ -------------------------- 122.4/390.2 MB 11.6 MB/s eta 0:00:24\n",
      "   ------------ -------------------------- 124.8/390.2 MB 11.5 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 127.1/390.2 MB 11.5 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 129.5/390.2 MB 11.5 MB/s eta 0:00:23\n",
      "   ------------- ------------------------- 131.9/390.2 MB 11.5 MB/s eta 0:00:23\n",
      "   ------------- ------------------------- 134.0/390.2 MB 11.5 MB/s eta 0:00:23\n",
      "   ------------- ------------------------- 136.3/390.2 MB 11.5 MB/s eta 0:00:23\n",
      "   ------------- ------------------------- 138.7/390.2 MB 11.5 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 140.8/390.2 MB 11.4 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 143.1/390.2 MB 11.4 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 145.2/390.2 MB 11.4 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 147.6/390.2 MB 11.4 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 149.7/390.2 MB 11.4 MB/s eta 0:00:22\n",
      "   --------------- ----------------------- 151.8/390.2 MB 11.3 MB/s eta 0:00:22\n",
      "   --------------- ----------------------- 153.9/390.2 MB 11.3 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 156.2/390.2 MB 11.3 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 158.1/390.2 MB 11.2 MB/s eta 0:00:21\n",
      "   ---------------- ---------------------- 160.2/390.2 MB 11.2 MB/s eta 0:00:21\n",
      "   ---------------- ---------------------- 162.5/390.2 MB 11.2 MB/s eta 0:00:21\n",
      "   ---------------- ---------------------- 164.6/390.2 MB 11.2 MB/s eta 0:00:21\n",
      "   ---------------- ---------------------- 167.0/390.2 MB 11.2 MB/s eta 0:00:20\n",
      "   ---------------- ---------------------- 169.3/390.2 MB 11.2 MB/s eta 0:00:20\n",
      "   ----------------- --------------------- 171.7/390.2 MB 11.2 MB/s eta 0:00:20\n",
      "   ----------------- --------------------- 174.3/390.2 MB 11.2 MB/s eta 0:00:20\n",
      "   ----------------- --------------------- 176.7/390.2 MB 11.2 MB/s eta 0:00:20\n",
      "   ----------------- --------------------- 179.3/390.2 MB 11.2 MB/s eta 0:00:19\n",
      "   ------------------ -------------------- 181.7/390.2 MB 11.2 MB/s eta 0:00:19\n",
      "   ------------------ -------------------- 184.3/390.2 MB 11.2 MB/s eta 0:00:19\n",
      "   ------------------ -------------------- 187.2/390.2 MB 11.2 MB/s eta 0:00:19\n",
      "   ------------------ -------------------- 189.5/390.2 MB 11.2 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 192.2/390.2 MB 11.2 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 194.8/390.2 MB 11.2 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 197.1/390.2 MB 11.2 MB/s eta 0:00:18\n",
      "   ------------------- ------------------- 199.8/390.2 MB 11.2 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 202.1/390.2 MB 11.3 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 204.7/390.2 MB 11.3 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 207.1/390.2 MB 11.3 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 209.7/390.2 MB 11.3 MB/s eta 0:00:17\n",
      "   --------------------- ----------------- 212.1/390.2 MB 11.3 MB/s eta 0:00:16\n",
      "   --------------------- ----------------- 214.4/390.2 MB 11.3 MB/s eta 0:00:16\n",
      "   --------------------- ----------------- 217.1/390.2 MB 11.3 MB/s eta 0:00:16\n",
      "   --------------------- ----------------- 219.4/390.2 MB 11.3 MB/s eta 0:00:16\n",
      "   ---------------------- ---------------- 222.0/390.2 MB 11.3 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 224.4/390.2 MB 11.3 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 227.0/390.2 MB 11.3 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 229.4/390.2 MB 11.3 MB/s eta 0:00:15\n",
      "   ----------------------- --------------- 231.7/390.2 MB 11.3 MB/s eta 0:00:15\n",
      "   ----------------------- --------------- 234.1/390.2 MB 11.3 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 236.5/390.2 MB 11.3 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 238.8/390.2 MB 11.3 MB/s eta 0:00:14\n",
      "   ------------------------ -------------- 241.4/390.2 MB 11.3 MB/s eta 0:00:14\n",
      "   ------------------------ -------------- 243.8/390.2 MB 11.3 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 246.2/390.2 MB 11.3 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 248.8/390.2 MB 11.3 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 251.1/390.2 MB 11.3 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 253.8/390.2 MB 11.3 MB/s eta 0:00:13\n",
      "   ------------------------- ------------- 256.4/390.2 MB 11.4 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 259.0/390.2 MB 11.4 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 261.4/390.2 MB 11.4 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 264.0/390.2 MB 11.4 MB/s eta 0:00:12\n",
      "   -------------------------- ------------ 266.6/390.2 MB 11.4 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 269.0/390.2 MB 11.4 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 271.3/390.2 MB 11.4 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 273.9/390.2 MB 11.4 MB/s eta 0:00:11\n",
      "   --------------------------- ----------- 276.6/390.2 MB 11.4 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 278.9/390.2 MB 11.4 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 281.5/390.2 MB 11.4 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 283.9/390.2 MB 11.4 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 286.5/390.2 MB 11.4 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 289.1/390.2 MB 11.4 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 291.2/390.2 MB 11.4 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 293.9/390.2 MB 11.4 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 296.5/390.2 MB 11.4 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 298.8/390.2 MB 11.4 MB/s eta 0:00:09\n",
      "   ------------------------------ -------- 301.5/390.2 MB 11.4 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 303.8/390.2 MB 11.4 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 306.4/390.2 MB 11.4 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 308.8/390.2 MB 11.4 MB/s eta 0:00:08\n",
      "   ------------------------------- ------- 311.4/390.2 MB 11.4 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 313.8/390.2 MB 11.4 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 316.1/390.2 MB 11.4 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 318.5/390.2 MB 11.4 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 321.1/390.2 MB 11.4 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 323.5/390.2 MB 11.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 325.8/390.2 MB 11.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 328.5/390.2 MB 11.4 MB/s eta 0:00:06\n",
      "   --------------------------------- ----- 330.8/390.2 MB 11.4 MB/s eta 0:00:06\n",
      "   --------------------------------- ----- 333.4/390.2 MB 11.4 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 335.8/390.2 MB 11.4 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 338.4/390.2 MB 11.4 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 340.8/390.2 MB 11.4 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 343.4/390.2 MB 11.4 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 346.0/390.2 MB 11.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 348.7/390.2 MB 11.4 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 351.0/390.2 MB 11.4 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 353.4/390.2 MB 11.4 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 355.7/390.2 MB 11.4 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 358.1/390.2 MB 11.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 360.7/390.2 MB 11.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 363.1/390.2 MB 11.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 365.7/390.2 MB 11.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 368.1/390.2 MB 11.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 370.7/390.2 MB 11.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 373.3/390.2 MB 11.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 375.7/390.2 MB 11.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 378.3/390.2 MB 11.4 MB/s eta 0:00:02\n",
      "   --------------------------------------  380.6/390.2 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  383.3/390.2 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.6/390.2 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  388.2/390.2 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 390.2/390.2 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.12.1-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 2.4/3.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 11.0 MB/s eta 0:00:00\n",
      "Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.4/26.4 MB 11.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 5.0/26.4 MB 11.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.3/26.4 MB 11.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 10.0/26.4 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.3/26.4 MB 11.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.7/26.4 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 17.0/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.4/26.4 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.8/26.4 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.1/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.4 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl (126 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.2 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp311-cp311-win_amd64.whl (292 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wheel, termcolor, tensorflow-io-gcs-filesystem, optree, opt-einsum, ml-dtypes, h5py, google-pasta, gast, astunparse, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 h5py-3.12.1 keras-3.7.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 tensorflow-2.18.0 tensorflow-intel-2.18.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.5.0 wheel-0.45.1\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785404, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>314</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>439</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>588</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1169</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1185</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id  user_id  rating\n",
       "0        1      314       5\n",
       "1        1      439       3\n",
       "2        1      588       5\n",
       "3        1     1169       4\n",
       "4        1     1185       4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_books = df.book_id.unique()\n",
    "len(n_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53424"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique()\n",
    "len(n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "53424\n"
     ]
    }
   ],
   "source": [
    "n_books = df[\"book_id\"].nunique()\n",
    "print(n_books)\n",
    "n_users = df[\"user_id\"].nunique()\n",
    "print(n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_input = Input(shape=[1], name=\"Book-Input\")\n",
    "book_embedding = Embedding(n_books+1, 5, name=\"Book-Embedding\")(book_input)\n",
    "book_vec = Flatten(name=\"Flatten-Books\")(book_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=[1], name=\"User-Input\")\n",
    "user_embedding = Embedding(n_users+1, 5, name=\"User-Embedding\")(user_input)\n",
    "user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = Concatenate()([book_vec, user_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = Dense(128, activation='relu')(conc)\n",
    "fc2 = Dense(32, activation='relu')(fc1)\n",
    "out = Dense(1)(fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model([user_input, book_input], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'adam',loss =  'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m24544/24544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 4ms/step - loss: 0.9973\n",
      "Epoch 2/5\n",
      "\u001b[1m24544/24544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 4ms/step - loss: 0.6809\n",
      "Epoch 3/5\n",
      "\u001b[1m24544/24544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 4ms/step - loss: 0.6459\n",
      "Epoch 4/5\n",
      "\u001b[1m24544/24544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 4ms/step - loss: 0.6203\n",
      "Epoch 5/5\n",
      "\u001b[1m24544/24544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 4ms/step - loss: 0.5967\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit([train.user_id, train.book_id], train.rating, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate([test.user_id, test.book_id], test.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = Dense(128, activation='relu')(conc)\n",
    "fc2 = Dense(32, activation='relu')(fc1)\n",
    "fc3 = Dense(8, activation='relu')(fc2)\n",
    "out = Dense(1)(fc3)\n",
    "\n",
    "model2 = Model([user_input, book_input], out)\n",
    "model2.compile('adam', 'mean_squared_error')\n",
    "result = model2.fit([train.user_id, train.book_id], train.rating, epochs=10, verbose=1)\n",
    "model2.evaluate([test.user_id, test.book_id], test.rating)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
